# based on https://medium.com/@yuxiaojian/host-your-own-ollama-service-in-a-cloud-kubernetes-k8s-cluster-c818ca84a055

# Define the ollama namespace
apiVersion: v1
kind: Namespace
metadata:
  name: webui-ollama-bundle
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-data-pvc
  labels:
    app: open-webui
spec:
  accessModes:
    - ReadWriteOnce # This means the volume can be mounted as read-write by a single node.
  resources:
    requests:
      storage: 500Gi # Adjust this size based on your expected Ollama model storage needs.
  storageClassName: local-path # k3s typically uses 'local-path' as its default StorageClass.
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: open-webui-data-pvc
  labels:
    app: open-webui
spec:
  accessModes:
    - ReadWriteOnce # This means the volume can be mounted as read-write by a single node.
  resources:
    requests:
      storage: 5Gi # Adjust this size based on your expected Open WebUI data storage needs.
  storageClassName: local-path # k3s typically uses 'local-path' as its default StorageClass.
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: open-webui-deployment
  labels:
    app: open-webui
spec:
  replicas: 1 # We want one instance of the Open WebUI container running.
  selector:
    matchLabels:
      app: open-webui
  template:
    metadata:
      labels:
        app: open-webui
    spec:
      # This is crucial for GPU access on your Jetson Orin Nano.
      # It assumes that the 'nvidia-container-runtime' is configured
      # as a runtime in your k3s environment and named 'nvidia'.
      # You might need to install the NVIDIA device plugin or configure k3s
      # to use this runtime if you haven't already.
      runtimeClassName: nvidia
      containers:
        - name: open-webui
          image: ghcr.io/open-webui/open-webui:ollama # The Docker image you specified.
          ports:
            - containerPort: 8080 # The port the application listens on inside the container.
          volumeMounts:
            - name: ollama-data-storage
              mountPath: /root/.ollama # Mounts the Ollama data PVC to this path.
            - name: open-webui-data-storage
              mountPath: /app/backend/data # Mounts the Open WebUI data PVC to this path.
          resources:
            limits:
              # Requesting 1 NVIDIA GPU. This tells Kubernetes to schedule this pod
              # on a node that has an available NVIDIA GPU.
              nvidia.com/gpu: 1
            requests:
              # It's good practice to also request resources, even if small,
              # to help with scheduling.
              cpu: "1000m" # 1 CPU core
              memory: "2Gi" # 2 GB of memory
      volumes:
        - name: ollama-data-storage
          persistentVolumeClaim:
            claimName: ollama-data-pvc # Refers to the PVC defined above.
        - name: open-webui-data-storage
          persistentVolumeClaim:
            claimName: open-webui-data-pvc # Refers to the PVC defined above.
---
apiVersion: v1
kind: Service
metadata:
  name: open-webui-service
  labels:
    app: open-webui
spec:
  selector:
    app: open-webui # This service will target pods with the label 'app: open-webui'.
  type: NodePort # Exposes the service on a port on each node's IP.
  ports:
    - protocol: TCP
      port: 3000 # The port that the service will be accessible on within the cluster.
      targetPort: 8080 # The port on the container that the service forwards traffic to.
      nodePort: 3000 # This explicitly sets the host port to 3000, matching your docker command.
                    # If port 3000 is already in use on your node, k3s might not start this service.
                    # You can remove 'nodePort: 3000' to let k3s assign a random available port (usually 30000-32767).
                    # You would then find the assigned port using `kubectl get svc open-webui-service`.
